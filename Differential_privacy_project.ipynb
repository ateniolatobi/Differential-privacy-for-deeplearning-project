{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Differential_privacy_project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDXBupQLTvSg",
        "colab_type": "text"
      },
      "source": [
        "#LESSON 6: DIFFERENTIAL PRIVACY FOR DEEPLEARNING PROJECT ON THE MNIST DATASET\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAtfkUya_Dva",
        "colab_type": "text"
      },
      "source": [
        "**Author** \n",
        ": Ateniola Oluwatobi Victor\n",
        "\n",
        "**Objective** : My implementation of the Final project in the fDifferential privacy in Deep learning  section of the Secure and Private AI Scholarship Challenge Nanodegree Program using the MNIST digit dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFvw1MO1MbwA",
        "colab_type": "code",
        "outputId": "82fb6c78-706d-4103-e503-2640837915bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        }
      },
      "source": [
        "!pip install syft"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: syft in /usr/local/lib/python3.6/dist-packages (0.1.21a1)\n",
            "Requirement already satisfied: msgpack>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from syft) (0.6.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.16.4)\n",
            "Requirement already satisfied: lz4>=2.1.6 in /usr/local/lib/python3.6/dist-packages (from syft) (2.1.10)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.0)\n",
            "Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.3.0)\n",
            "Requirement already satisfied: flask-socketio>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from syft) (4.2.0)\n",
            "Requirement already satisfied: zstd>=1.4.0.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.1.0)\n",
            "Requirement already satisfied: tf-encrypted>=0.5.4 in /usr/local/lib/python3.6/dist-packages (from syft) (0.5.7)\n",
            "Requirement already satisfied: websockets>=7.0 in /usr/local/lib/python3.6/dist-packages (from syft) (8.0.2)\n",
            "Requirement already satisfied: Flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.1)\n",
            "Requirement already satisfied: tblib>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.21.2)\n",
            "Requirement already satisfied: websocket-client>=0.56.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.56.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->syft) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->syft) (4.3.0)\n",
            "Requirement already satisfied: python-socketio>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from flask-socketio>=3.3.2->syft) (4.3.0)\n",
            "Requirement already satisfied: tensorflow<2,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted>=0.5.4->syft) (5.1.2)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (7.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (2.10.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (0.15.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (0.13.2)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision>=0.3.0->syft) (0.46)\n",
            "Requirement already satisfied: python-engineio>=3.9.0 in /usr/local/lib/python3.6/dist-packages (from python-socketio>=4.3.0->flask-socketio>=3.3.2->syft) (3.9.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.7.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (3.7.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.33.4)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.2.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=1.0.2->syft) (1.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l0ome4u0FDM",
        "colab_type": "code",
        "outputId": "640844ee-03bf-4265-afb4-ef0fcc600a17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import Subset\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from syft.frameworks.torch.differential_privacy import pate\n",
        "import helper\n",
        "\n",
        "# Switch between cpu and gpu depending on which is available for training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0802 14:37:46.335293 140178328717184 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
            "W0802 14:37:46.352915 140178328717184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBHHNURs0n02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Application of transforms to normalize the mnist data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wzDg4VI2Q3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method to divide the mnist trainingset among the number of teachers to simulate unique datasets \n",
        "def private_data_loaders(trainset, teachers):\n",
        "  num_part = len(trainset) // teachers\n",
        "  \n",
        "  priv_loaders = []\n",
        "  for i in range(teachers):\n",
        "    indices = list(range(i * num_part, (i + 1)*num_part)) \n",
        "    if (i == teachers - 1):\n",
        "      indices = list(range(i * num_part, len(trainset)))\n",
        "    sub_pd = Subset(trainset, indices)\n",
        "    temp_loader = torch.utils.data.DataLoader(sub_pd, batch_size=64, shuffle=True)\n",
        "    priv_loaders.append(temp_loader)\n",
        "  return priv_loaders"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR3qEOIfXDVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method for creating and training a model\n",
        "def create_train_model(classifier, loader, lr = 0.12, epoch = 100):\n",
        "  print(\"Running on \", device)\n",
        "  model = classifier()\n",
        "  optimizer = optim.SGD(model.parameters(), lr)\n",
        "  \n",
        "  criterion = nn.NLLLoss()\n",
        "  \n",
        "  model.to(device)\n",
        "  for i in range(epoch):\n",
        "    cum_loss  = 0\n",
        "    cum_perc = 0\n",
        "    for imgs, labels in loader:\n",
        "      imgs, labels = imgs.to(device), labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      output = model.forward(imgs)\n",
        "      loss = criterion(output, labels)\n",
        "      loss.backward()\n",
        "      cum_loss +=  loss.item()\n",
        "      optimizer.step()\n",
        "    for imgs, labels in loader:\n",
        "      imgs, labels = imgs.to(device), labels.to(device)\n",
        "      with torch.no_grad():\n",
        "        ps =  torch.exp(model.forward(imgs))\n",
        "      top_p, top_class = ps.topk(1, dim = 1)\n",
        "      prob = top_class == labels.view(*top_class.shape)\n",
        "      prob = prob.float()\n",
        "      cum_perc += prob.mean().float()\n",
        "    if (i == epoch -1):\n",
        "      print(\"The loss for {0} epoch is {1}\".format(i, cum_loss / len(loader)))\n",
        "      print(\"The percentage for {0} epoch is {1}\".format(i, cum_perc / len(loader)))  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XCLCVwP63ue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method for seperating the mnist test dataset into 2. The first one being the public database and the other one the private database\n",
        "def test_database_seperator(testset):\n",
        "  i1 = int(len(testset) * 0.9)\n",
        "  i2 = int(len(testset) * 0.1)\n",
        "  \n",
        "  ind1 = list(range(0, i1))\n",
        "  ind2 = list(range(i1, len(testset)))\n",
        "  \n",
        "  pdb = Subset(testset, ind1)\n",
        "  \n",
        "  db = Subset(testset, ind2)\n",
        "  \n",
        "  pdb_loader = torch.utils.data.DataLoader(pdb, batch_size=64, shuffle=False)\n",
        "  \n",
        "  db_loader = torch.utils.data.DataLoader(db, batch_size=64, shuffle=True)\n",
        "  return pdb_loader, db_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqtJGOmXgKzv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method for running the unlabelled database through the teacher models in order to get their respective predictions for each items.\n",
        "def evaluate(models, loader):\n",
        "  m_labels = []\n",
        "  for model in models:\n",
        "    model_class = np.array([[30]])\n",
        "    for imgs, labels in loader:\n",
        "      imgs = imgs.to(device)\n",
        "      with torch.no_grad():\n",
        "        ps =  torch.exp(model.forward(imgs))\n",
        "      top_p, top_class = ps.topk(1, dim = 1)\n",
        "      model_class = np.append(model_class, np.array(top_class.cpu()))\n",
        "    m_label = np.delete(model_class, 0, axis = 0)\n",
        "    m_labels.append(m_label)\n",
        "  return m_labels\n",
        "  \n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHac_w6n-Czo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Classifier for creating the models\n",
        "class classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__() \n",
        "    self.fc1 = nn.Linear(784, 256)\n",
        "    self.fc2 = nn.Linear(256, 128)\n",
        "    self.fc3 = nn.Linear(128, 64)\n",
        "    self.fc4 = nn.Linear(64, 32)\n",
        "    self.fc5 = nn.Linear(32, 10)\n",
        "    \n",
        "    self.dropout = nn.Dropout(p = 0.2)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    x = self.dropout(F.relu(self.fc1(x)))\n",
        "    x = self.dropout(F.relu(self.fc2(x)))\n",
        "    x = self.dropout(F.relu(self.fc3(x)))\n",
        "    x = self.dropout(F.relu(self.fc4(x)))\n",
        "    x = F.log_softmax(self.fc5(x), dim = 1)   \n",
        "    return x\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5NSAWLva4Rf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl69hfZp_PCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method for creating and training the teacher models\n",
        "def train_teacher_models(loaders, lr = 0.12, epoch = 10):\n",
        "  teacher_models = []\n",
        "  for loader in loaders:\n",
        "    model = create_train_model(classifier, loader, lr, epoch)\n",
        "    teacher_models.append(model)\n",
        "  return teacher_models\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n6PrCoNTacpG",
        "colab": {}
      },
      "source": [
        "#Method for applying Global differential privacy to the labels predicted by the teacher models and to perform PATE analysis.\n",
        "def return_new_indices(preds, epsilon):\n",
        "  preds = preds.T\n",
        "  ind = []\n",
        "  beta = 1 / epsilon\n",
        "  for pred in preds:\n",
        "    label_count = np.bincount(pred, minlength = 10)\n",
        "    for i in range(len(label_count)):\n",
        "      label_count[i] += np.random.laplace(0, beta, 1)\n",
        "    new_labels = np.argmax(label_count)\n",
        "    ind.append(new_labels)\n",
        "\n",
        "  ind = np.array(ind)\n",
        "  return ind\n",
        "\n",
        "\n",
        "def pate_analysis(pred, ind, epsilon):\n",
        "  dde, die = pate.perform_analysis(teacher_preds = pred, indices = ind, noise_eps = epsilon, delta = 1e-5 )\n",
        "  print(\"Data dependent epsilon \", dde)\n",
        "  print(\"Data Independent epsilon \", die)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC7NAXyzhnPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Method to create a new dataloader with the new target labels and the public database\n",
        "def join_label_image(dataloader, ind):\n",
        "  img_list = []\n",
        "  for img,label in dataloader:\n",
        "    img_list.append(img)\n",
        "\n",
        "  images = np.vstack(img_list)\n",
        "\n",
        "  model_zip = list(zip(images, ind))\n",
        "  modelloader = torch.utils.data.DataLoader(model_zip, shuffle=True, batch_size=64)\n",
        "  return modelloader\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teFfYd71qMxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Method o analyze the private database with the trained model\n",
        "def analyze_privatedata(model, loader):\n",
        "  print(\"Running on \", device)\n",
        "  model.to(device)\n",
        "  cum_perc = 0\n",
        "  for imgs, labels in loader:\n",
        "    imgs, labels = imgs.to(device), labels.to(device)\n",
        "    with torch.no_grad():\n",
        "      ps =  torch.exp(model.forward(imgs))\n",
        "    top_p, top_class = ps.topk(1, dim = 1)\n",
        "    prob = top_class == labels.view(*top_class.shape)\n",
        "    prob = prob.float()\n",
        "    cum_perc += prob.mean().float()\n",
        "  print(\"The accuracy of the differentially private model on the private dataset is {0}%\".format((cum_perc / len(loader)) * 100))  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3huTVjK4D80y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teachers = 10\n",
        "epsilon = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnk-_AypskaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pdb, db = test_database_seperator(mnist_testset)\n",
        "teachers_loaders = private_data_loaders(mnist_trainset, teachers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Wbjyyq-vLas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_models = train_teacher_models(teachers_loaders, lr = 0.12, epoch = 40)\n",
        "teachers_pred = np.array(evaluate(teacher_models, pdb))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "213T9kM3vX-R",
        "colab_type": "code",
        "outputId": "aacadc2f-9d98-415e-c4f4-ba8aeb2b3288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "indices = return_new_indices(teachers_pred, epsilon)\n",
        "pate_analysis(teachers_pred, indices, epsilon)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data dependent epsilon  11.211405609064586\n",
            "Data Independent epsilon  371.5129254649703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxcyR7ecvcvR",
        "colab_type": "code",
        "outputId": "6ecd78be-f831-4c35-bae3-5145a94cd50f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "labelledloader = join_label_image(pdb, indices)\n",
        "main_model = create_train_model(classifier, labelledloader, lr = 0.06, epoch = 30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on  cuda\n",
            "The loss for 29 epoch is 0.10234993453144181\n",
            "The percentage for 29 epoch is 0.9759308099746704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-mdHkRLvdiZ",
        "colab_type": "code",
        "outputId": "0f4dd852-112b-4b39-d8ab-f311cf59d494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "analyze_privatedata(main_model, db)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on  cuda\n",
            "The accuracy of the differentially private model on the private dataset is 92.3046875%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1Pzn0x5NdR5",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}
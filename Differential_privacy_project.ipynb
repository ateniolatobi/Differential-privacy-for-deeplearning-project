{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Differential_privacy_project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDXBupQLTvSg",
        "colab_type": "text"
      },
      "source": [
        "#LESSON 6: DIFFERENTIAL PRIVACY FOR DEEPLEARNING PROJECT ON THE MNIST DATASET\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAtfkUya_Dva",
        "colab_type": "text"
      },
      "source": [
        "**Author** \n",
        ": Ateniola Oluwatobi Victor\n",
        "\n",
        "**Objective** : My implementation of the Final project in the fDifferential privacy in Deep learning  section of the Secure and Private AI Scholarship Challenge Nanodegree Program using the MNIST digit dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFvw1MO1MbwA",
        "colab_type": "code",
        "outputId": "9c87794e-269f-4a7a-9c61-7d52f4a571d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install syft"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting syft\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/2e/16bdefc78eb089e1efa9704c33b8f76f035a30dc935bedd7cbb22f6dabaa/syft-0.1.21a1-py3-none-any.whl (219kB)\n",
            "\r\u001b[K     |█▌                              | 10kB 17.6MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 24.1MB/s eta 0:00:01\r\u001b[K     |████▌                           | 30kB 30.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 40kB 34.9MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 51kB 38.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 61kB 40.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 71kB 41.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 81kB 41.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 92kB 43.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 102kB 44.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 112kB 44.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 122kB 44.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 133kB 44.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 143kB 44.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 153kB 44.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 163kB 44.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 174kB 44.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 184kB 44.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 194kB 44.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 204kB 44.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 215kB 44.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 225kB 44.6MB/s \n",
            "\u001b[?25hCollecting msgpack>=0.6.1 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/7e/ae9e91c1bb8d846efafd1f353476e3fd7309778b582d2fb4cea4cc15b9a2/msgpack-0.6.1-cp36-cp36m-manylinux1_x86_64.whl (248kB)\n",
            "\r\u001b[K     |█▎                              | 10kB 24.8MB/s eta 0:00:01\r\u001b[K     |██▋                             | 20kB 34.6MB/s eta 0:00:01\r\u001b[K     |████                            | 30kB 42.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 40kB 44.9MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 51kB 46.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 61kB 46.4MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 71kB 47.6MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 81kB 48.8MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 92kB 50.9MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 102kB 52.4MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 112kB 52.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 122kB 52.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 133kB 52.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 143kB 52.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 153kB 52.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 163kB 52.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 174kB 52.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 184kB 52.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 194kB 52.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 204kB 52.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 215kB 52.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 225kB 52.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 235kB 52.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 245kB 52.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 256kB 52.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tblib>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.0)\n",
            "Collecting lz4>=2.1.6 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/c6/96bbb3525a63ebc53ea700cc7d37ab9045542d33b4d262d0f0408ad9bbf2/lz4-2.1.10-cp36-cp36m-manylinux1_x86_64.whl (385kB)\n",
            "\r\u001b[K     |▉                               | 10kB 23.0MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 30.2MB/s eta 0:00:01\r\u001b[K     |██▌                             | 30kB 36.2MB/s eta 0:00:01\r\u001b[K     |███▍                            | 40kB 41.1MB/s eta 0:00:01\r\u001b[K     |████▎                           | 51kB 43.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 61kB 45.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 71kB 46.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 81kB 45.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 92kB 47.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 102kB 48.5MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 112kB 48.5MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 122kB 48.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 133kB 48.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 143kB 48.5MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 153kB 48.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 163kB 48.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 174kB 48.5MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 184kB 48.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 194kB 48.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 204kB 48.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 215kB 48.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 225kB 48.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 235kB 48.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 245kB 48.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 256kB 48.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 266kB 48.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 276kB 48.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 286kB 48.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 296kB 48.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 307kB 48.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 317kB 48.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 327kB 48.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 337kB 48.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 348kB 48.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 358kB 48.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 368kB 48.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 378kB 48.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 389kB 48.5MB/s \n",
            "\u001b[?25hCollecting tf-encrypted>=0.5.4 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/ff/7dbd5fc77fcec0df1798268a6b72a2ab0150b854761bc39c77d566798f0b/tf_encrypted-0.5.7-py3-none-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 51.3MB/s \n",
            "\u001b[?25hCollecting websocket-client>=0.56.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 58.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.0)\n",
            "Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.3.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.16.4)\n",
            "Requirement already satisfied: Flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.21.3)\n",
            "Collecting flask-socketio>=3.3.2 (from syft)\n",
            "  Downloading https://files.pythonhosted.org/packages/33/31/f779e69e59f528684d8c9925b3c82a9303d148655d9671ba2975ab8c3894/Flask_SocketIO-4.2.0-py2.py3-none-any.whl\n",
            "Collecting websockets>=7.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/4b/ad228451b1c071c5c52616b7d4298ebcfcac5ae8515ede959db19e4cd56d/websockets-8.0.2-cp36-cp36m-manylinux1_x86_64.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 33.8MB/s \n",
            "\u001b[?25hCollecting zstd>=1.4.0.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/37/6a7ba746ebddbd6cd06de84367515d6bc239acd94fb3e0b1c85788176ca2/zstd-1.4.1.0.tar.gz (454kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 47.7MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1 (from tf-encrypted>=0.5.4->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/e8/b3212641ee2718d556df0f23f78de8303f068fe29cdaa7a91018849582fe/PyYAML-5.1.2.tar.gz (265kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 52.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.56.0->syft) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->syft) (4.3.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (0.15.5)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (7.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (2.10.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (0.13.2)\n",
            "Collecting python-socketio>=4.3.0 (from flask-socketio>=3.3.2->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/5a/9429c1fbc399b6079725150a36491efd6bd4691c11110f5a57e8c991de96/python_socketio-4.3.0-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 27.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.1.7)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.33.4)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (3.7.1)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision>=0.3.0->syft) (0.46)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=1.0.2->syft) (1.1.1)\n",
            "Collecting python-engineio>=3.9.0 (from python-socketio>=4.3.0->flask-socketio>=3.3.2->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/ba/5a689b07d399cd91cd91875232a1af8a63f0bd2cd0d0898da295f127544e/python_engineio-3.9.2-py2.py3-none-any.whl (119kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 56.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (3.1.1)\n",
            "Building wheels for collected packages: zstd, pyyaml\n",
            "  Building wheel for zstd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zstd: filename=zstd-1.4.1.0-cp36-cp36m-linux_x86_64.whl size=1067082 sha256=9349fb1889cca9bd4ee86b0c6d9d99fa05ad9d01786287e9f44079cfeb085fec\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/3f/ee/ac08c81af7c1b24a80c746df669ea3cb37542d27877d66ccf4\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1.2-cp36-cp36m-linux_x86_64.whl size=44105 sha256=2b737248b022dbd158fb3599400247c857ea90dc27128299ac7f25b37d13bb77\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/45/dd/65f0b38450c47cf7e5312883deb97d065e030c5cca0a365030\n",
            "Successfully built zstd pyyaml\n",
            "Installing collected packages: msgpack, lz4, pyyaml, tf-encrypted, websocket-client, python-engineio, python-socketio, flask-socketio, websockets, zstd, syft\n",
            "  Found existing installation: msgpack 0.5.6\n",
            "    Uninstalling msgpack-0.5.6:\n",
            "      Successfully uninstalled msgpack-0.5.6\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed flask-socketio-4.2.0 lz4-2.1.10 msgpack-0.6.1 python-engineio-3.9.2 python-socketio-4.3.0 pyyaml-5.1.2 syft-0.1.21a1 tf-encrypted-0.5.7 websocket-client-0.56.0 websockets-8.0.2 zstd-1.4.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l0ome4u0FDM",
        "colab_type": "code",
        "outputId": "2259b1d8-3f69-4fd6-9d15-37cb54b19d07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import Subset\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from syft.frameworks.torch.differential_privacy import pate\n",
        "import helper\n",
        "\n",
        "# Switch between cpu and gpu depending on which is available for training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0804 13:49:44.453457 140375643936640 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
            "W0804 13:49:44.468619 140375643936640 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBHHNURs0n02",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "58c2f8aa-096c-4ee3-852e-dda837c04f1f"
      },
      "source": [
        "# Application of transforms to normalize the mnist data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:03, 3195332.82it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 46253.55it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:02, 813804.74it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 18205.85it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wzDg4VI2Q3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method to divide the mnist trainingset among the number of teachers to simulate unique datasets \n",
        "def private_data_loaders(trainset, teachers):\n",
        "  num_part = len(trainset) // teachers\n",
        "  \n",
        "  priv_loaders = []\n",
        "  for i in range(teachers):\n",
        "    indices = list(range(i * num_part, (i + 1)*num_part)) \n",
        "#     if (i == teachers - 1):\n",
        "#       indices = list(range(i * num_part, len(trainset)))\n",
        "    sub_pd = Subset(trainset, indices)\n",
        "    temp_loader = torch.utils.data.DataLoader(sub_pd, batch_size=64, shuffle=True)\n",
        "    priv_loaders.append(temp_loader)\n",
        "  return priv_loaders"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XCLCVwP63ue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method for seperating the mnist test dataset into 2. The first one being the public database and the other one the private database\n",
        "def test_database_seperator(testset):\n",
        "  i1 = int(len(testset) * 0.9)\n",
        "  i2 = int(len(testset) * 0.1)\n",
        "  \n",
        "  ind1 = list(range(0, i1))\n",
        "  ind2 = list(range(i1, len(testset)))\n",
        "  \n",
        "  pdb = Subset(testset, ind1)\n",
        "  \n",
        "  db = Subset(testset, ind2)\n",
        "  \n",
        "  pdb_loader = torch.utils.data.DataLoader(pdb, batch_size=64, shuffle=False)\n",
        "  \n",
        "  db_loader = torch.utils.data.DataLoader(db, batch_size=64, shuffle=True)\n",
        "  return pdb_loader, db_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR3qEOIfXDVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method for creating and training a model\n",
        "def create_train_model(classifier, loader, lr = 0.12, epoch = 100):\n",
        "  print(\"Running on \", device)\n",
        "  model = classifier()\n",
        "  optimizer = optim.SGD(model.parameters(), lr)\n",
        "  \n",
        "  criterion = nn.NLLLoss()\n",
        "  \n",
        "  model.to(device)\n",
        "  for i in range(epoch):\n",
        "    cum_loss  = 0\n",
        "    cum_perc = 0\n",
        "    for imgs, labels in loader:\n",
        "      imgs, labels = imgs.to(device), labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      output = model.forward(imgs)\n",
        "      loss = criterion(output, labels)\n",
        "      loss.backward()\n",
        "      cum_loss +=  loss.item()\n",
        "      optimizer.step()\n",
        "    for imgs, labels in loader:\n",
        "      imgs, labels = imgs.to(device), labels.to(device)\n",
        "      with torch.no_grad():\n",
        "        ps =  torch.exp(model.forward(imgs))\n",
        "      top_p, top_class = ps.topk(1, dim = 1)\n",
        "      prob = top_class == labels.view(*top_class.shape)\n",
        "      prob = prob.float()\n",
        "      cum_perc += prob.mean().float()\n",
        "    if (i == epoch -1):\n",
        "      print(\"The loss for {0} epoch is {1}\".format(i, cum_loss / len(loader)))\n",
        "      print(\"The percentage for {0} epoch is {1}\".format(i, cum_perc / len(loader)))  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqtJGOmXgKzv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method for running the unlabelled database through the teacher models in order to get their respective predictions for each items.\n",
        "\n",
        "def evaluate(models, loader):\n",
        "  m_labels = []\n",
        "  for model in models:\n",
        "    model_class = []\n",
        "    for imgs, labels in loader:\n",
        "      imgs = imgs.to(device)\n",
        "      with torch.no_grad():\n",
        "        ps =  torch.exp(model.forward(imgs))\n",
        "      top_p, top_class = ps.topk(1, dim = 1)\n",
        "      \n",
        "      model_class.append(np.array(top_class.cpu()).T)\n",
        "    m_label = np.hstack(model_class)\n",
        "    m_labels.append(m_label)\n",
        "  return m_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5NSAWLva4Rf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl69hfZp_PCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method for creating and training the teacher models\n",
        "def train_teacher_models(loaders, lr = 0.12, epoch = 10):\n",
        "  teacher_models = []\n",
        "  for loader in loaders:\n",
        "    model = create_train_model(classifier, loader, lr, epoch)\n",
        "    teacher_models.append(model)\n",
        "  return teacher_models\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n6PrCoNTacpG",
        "colab": {}
      },
      "source": [
        "#Method for applying Global differential privacy to the labels predicted by the teacher models and to perform PATE analysis.\n",
        "def return_new_indices(preds, epsilon):\n",
        "  preds = preds.T\n",
        "  ind = []\n",
        "  beta = 1 / epsilon\n",
        "  for pred in preds:\n",
        "    label_count = np.bincount(pred, minlength = 10)\n",
        "    for i in range(len(label_count)):\n",
        "      label_count[i] += np.random.laplace(0, beta, 1)\n",
        "    new_labels = np.argmax(label_count)\n",
        "    ind.append(new_labels)\n",
        "\n",
        "  ind = np.array(ind)\n",
        "  return ind\n",
        "\n",
        "\n",
        "def pate_analysis(pred, ind, epsilon):\n",
        "  dde, die = pate.perform_analysis(teacher_preds = pred, indices = ind, noise_eps = epsilon, delta = 1e-5 )\n",
        "  print(\"Data dependent epsilon \", dde)\n",
        "  print(\"Data Independent epsilon \", die)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC7NAXyzhnPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Method to create a new dataloader with the new target labels and the public database\n",
        "def join_label_image(dataloader, ind):\n",
        "  img_list = []\n",
        "  for img,label in dataloader:\n",
        "    img_list.append(img)\n",
        "\n",
        "  images = np.vstack(img_list)\n",
        "\n",
        "  model_zip = list(zip(images, ind))\n",
        "  modelloader = torch.utils.data.DataLoader(model_zip, shuffle=True, batch_size=64)\n",
        "  return modelloader\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teFfYd71qMxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Method o analyze the private database with the trained model\n",
        "def analyze_privatedata(model, loader):\n",
        "  print(\"Running on \", device)\n",
        "  model.to(device)\n",
        "  cum_perc = 0\n",
        "  for imgs, labels in loader:\n",
        "    imgs, labels = imgs.to(device), labels.to(device)\n",
        "    with torch.no_grad():\n",
        "      ps =  torch.exp(model.forward(imgs))\n",
        "    top_p, top_class = ps.topk(1, dim = 1)\n",
        "    prob = top_class == labels.view(*top_class.shape)\n",
        "    prob = prob.float()\n",
        "    cum_perc += prob.mean().float()\n",
        "  print(\"The accuracy of the differentially private model on the private dataset is {0}%\".format((cum_perc / len(loader)) * 100))  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHac_w6n-Czo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Classifier for creating the models\n",
        "class classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__() \n",
        "    self.fc1 = nn.Linear(784, 256)\n",
        "    self.fc2 = nn.Linear(256, 128)\n",
        "    self.fc3 = nn.Linear(128, 64)\n",
        "    self.fc4 = nn.Linear(64, 32)\n",
        "    self.fc5 = nn.Linear(32, 10)\n",
        "    \n",
        "    self.dropout = nn.Dropout(p = 0.2)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    x = self.dropout(F.relu(self.fc1(x)))\n",
        "    x = self.dropout(F.relu(self.fc2(x)))\n",
        "    x = self.dropout(F.relu(self.fc3(x)))\n",
        "    x = self.dropout(F.relu(self.fc4(x)))\n",
        "    x = F.log_softmax(self.fc5(x), dim = 1)   \n",
        "    return x\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3huTVjK4D80y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teachers = 100\n",
        "epsilon = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnk-_AypskaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pdb, db = test_database_seperator(mnist_testset)\n",
        "teachers_loaders = private_data_loaders(mnist_trainset, teachers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Wbjyyq-vLas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_models = train_teacher_models(teachers_loaders, lr = 0.12, epoch = 40)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3lwp6qPCixV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teachers_pred = np.array(evaluate(teacher_models, pdb))\n",
        "teachers_pred = teachers_pred.reshape(teachers, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "213T9kM3vX-R",
        "colab_type": "code",
        "outputId": "aacadc2f-9d98-415e-c4f4-ba8aeb2b3288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "indices = return_new_indices(teachers_pred, epsilon)\n",
        "pate_analysis(teachers_pred, indices, epsilon)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data dependent epsilon  11.211405609064586\n",
            "Data Independent epsilon  371.5129254649703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxcyR7ecvcvR",
        "colab_type": "code",
        "outputId": "6ecd78be-f831-4c35-bae3-5145a94cd50f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "labelledloader = join_label_image(pdb, indices)\n",
        "main_model = create_train_model(classifier, labelledloader, lr = 0.06, epoch = 30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on  cuda\n",
            "The loss for 29 epoch is 0.10234993453144181\n",
            "The percentage for 29 epoch is 0.9759308099746704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-mdHkRLvdiZ",
        "colab_type": "code",
        "outputId": "0f4dd852-112b-4b39-d8ab-f311cf59d494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "analyze_privatedata(main_model, db)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on  cuda\n",
            "The accuracy of the differentially private model on the private dataset is 92.3046875%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}